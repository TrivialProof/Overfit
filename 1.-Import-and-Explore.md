Import and Explore
================
Eoin Scanlon

# Import and sneak peak

Data can be downloaded from
<https://www.kaggle.com/c/dont-overfit-ii/data>. Always good to have a
look at the structure of the data first.

``` r
train <- read.csv(file = 'train.csv')
#head(train)
ncol(train)
```

    ## [1] 302

``` r
nrow(train)
```

    ## [1] 250

The column ‘target’ is the outcome that we will wish to predict, and the
rest are just arbitrary, meaningless data. The training dataset is much,
much smaller than the test. Interesting. Lets see some summary
statistics of the data.

``` r
#summary(train)
```

This is very hard to read, so lets see how to interpret this a bit
better.

``` r
train.data <- train[,-c(1,2)] # Remove unnecessary columns
train.means <- colMeans(train.data) # Means of each column
train.sd <- apply(train.data, 2, sd) # SD of each column

p1<- ggplot(as.data.frame(train.means), aes(x = train.means)) + geom_histogram(color="black", fill="blue",binwidth = 0.01)
p2<- ggplot(as.data.frame(train.sd), aes(x = train.sd)) + geom_histogram(color="black", fill="green",binwidth = 0.01)

grid.arrange(p1, p2, ncol = 2)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-3-1.png)<!-- -->

Ok so the means seem to be spread from about -0.2 to +0.2, with a normal
distribution type vibe. So lets draw a normal distribution over the
plots and see.

``` r
# Means
p1 <- ggplot(as.data.frame(train.means), aes(x = train.means)) + geom_histogram(aes(y =..density..),
                                                                          color="black", 
                                                                          fill="blue",
                                                                          binwidth = 0.01) + 
  stat_function(fun = dnorm, args = list(mean = mean(train.means), sd = sd(train.means)))

# Standard Deviations
p2 <- ggplot(as.data.frame(train.sd), aes(x = train.sd)) + geom_histogram(aes(y =..density..),
                                                                    color="black", 
                                                                    fill="green",
                                                                    binwidth = 0.01) +
  stat_function(fun = dnorm, args = list(mean = mean(train.sd), sd = sd(train.sd)))

grid.arrange(p1, p2, ncol = 2)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

## Testing for Normality

We can test whether these data are normally distributed using QQplots.

``` r
# Test for normality
qqnorm(train.means,main="QQ plot of means",pch=19)
qqline(train.means)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-5-1.png)<!-- -->

``` r
qqnorm(train.sd,main="QQ plot of SDs",pch=19)
qqline(train.sd)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-5-2.png)<!-- -->

They seem to be pretty consistent with normal distributions. But these
are just the *means* and the *standard deviations*. If we want to look
at the actual data, we should use something non-visual, since there are
so may columns. The Shapiro-Wilk test might be a good idea for single
column inputs, but here we can use a multivariate normality
test.

``` r
mvn(train.data,multivariatePlot = "qq")
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

So clearly, the data is not multivariate normal, or the points above
would lie along the straight line.

## Correlation Analysis

``` r
res <- cor(train.data)
#round(res, 2)
melted_data <- melt(res)
ggplot(data = melted_data, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-7-1.png)<!-- -->

At this scale, this looks like pure white noise. There may be
correlations within this data, but we just can’t see them here because
of the size issue. So lets look at a histogram of all the correlations.

``` r
head(melted_data)
```

    ##   Var1 Var2       value
    ## 1   X0   X0  1.00000000
    ## 2   X1   X0  0.03993948
    ## 3   X2   X0  0.06984606
    ## 4   X3   X0 -0.13982899
    ## 5   X4   X0  0.07936038
    ## 6   X5   X0 -0.06325927

``` r
ggplot(melted_data, aes(x = melted_data$value)) + geom_histogram(aes(y =..density..),
                                                                    color="black", 
                                                                    fill="yellow",
                                                                    binwidth = 0.01) + 
  stat_function(fun = dnorm, args = list(mean = mean(melted_data$value), sd = sd(melted_data$value)))
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

Pretty safe to say so that all of the data is uncorrelated. The
correlation values are almost a textbook normal distrubution with a mean
of 0 and small standard deviation. The small spike at 1 is the values’
self-correlations.

So lets draw a line under this now and assume (safely) that there is
very little correlation between columns, the data is not multivariate
normal and that the means/standard deviations *between* columns are
normally distributed.

# Further Data Exploration

First up, standardise the data. Its not really necessary, as the data is
so similar in terms of range. But still, can’t hurt.

``` r
scaled.dat <- scale(train.data)
```

## PCA

Lets look at a principle component analysis. Since the dataset is so
large (300 columns/variables), we would hope that we’ll discover that a
small number of components (eigenvectors) account for a significant
proportion of the data.

``` r
dat.pca <- prcomp(scaled.dat)
x <- summary(dat.pca)
x$importance[,1:10]
```

    ##                             PC1      PC2      PC3      PC4     PC5      PC6
    ## Standard deviation     2.110503 2.072794 2.033997 2.012024 1.97891 1.955954
    ## Proportion of Variance 0.014850 0.014320 0.013790 0.013490 0.01305 0.012750
    ## Cumulative Proportion  0.014850 0.029170 0.042960 0.056450 0.06951 0.082260
    ##                             PC7      PC8      PC9     PC10
    ## Standard deviation     1.945051 1.929739 1.912301 1.901555
    ## Proportion of Variance 0.012610 0.012410 0.012190 0.012050
    ## Cumulative Proportion  0.094870 0.107280 0.119470 0.131530

No such luck. It seems as though all 300 components are equally
‘important’. There doesn’t seem to be much here in terms of
eigenvector decomposition. From the output above, we can see that the
cumulative population of even the top 10 components accounts for only
around 13% of all variance in the data. Hardly useful.

## K-means clustering

We will have to first figure out the ‘correct’ (optimum) number of
clusters (k). To do so, we can implement a simple function that iterates
over a range of k and calculates their sihouette scores.

``` r
silhouette_score <- function(k){
  km <- kmeans(scaled.dat, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(scaled.dat))
  mean(ss[,3])
}
```

Run and plot the average silhouette scores.

``` r
k <- 2:20
avg_sil <- sapply(k, silhouette_score)
```

    ## Warning: did not converge in 10 iterations
    
    ## Warning: did not converge in 10 iterations
    
    ## Warning: did not converge in 10 iterations

``` r
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

It seems as though 16 may be the optimum, although that is only
relative. The actual silhouette score is low for k = 16, at around
0.012. This data set is really weird. Another method gives us the same
uncertainty.

``` r
fviz_nbclust(scaled.dat, kmeans, method='silhouette',k.max = 20)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-13-1.png)<!-- -->

However, model simplicity should always be favoured over complexity. I
won’t pursue the clustering method, as \> 16 or so clusters, with
marginally increasing signs of improvement leaves me with a bad feeling.

## K Nearest Neighbours (KNN)

As this is a supervised algorithm, we’ll need to break this training
dataset of 250 rows into its own train and test first.

``` r
set.seed(123)
# Add back in the target variable first
scaled.dat <- cbind(scaled.dat,train$target)

# Break up the data into train and test 
## Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(scaled.dat), 0.9 * nrow(scaled.dat)) 

## Extract training set
scaled.train <- scaled.dat[ran,] 
## Extract testing set
scaled.test <- scaled.dat[-ran,] 
```

Now run the k nearest neighbours for, say, 13 nearest neighbours. We’ll
try to optimise this
next.

``` r
pr <- knn(scaled.train[,0:300],scaled.test[,0:300],cl=scaled.train[,301],k=13)

##create confusion matrix
tab <- table(pr,scaled.test[,301])

##this function divides the correct predictions by total number of predictions that tell us how accurate the model is.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)
```

    ## [1] 76

This gives about 75% accuracy. But maybe there is better with a
different choice of k. Lets create a function similar to that in the
clustering above.

``` r
vector = c() # This is the empty vector where we will store our scores

neighbour_score <- function(k){
  pr <- knn(scaled.train[,0:300],scaled.test[,0:300],cl=scaled.train[,301],k)
  tab <- table(pr,scaled.test[,301])
  vector <- c(accuracy(tab))
}
k <- 2:20
avg_neighbour <- sapply(k, neighbour_score)
plot(k, type='b', avg_neighbour, xlab='Number of neighbours', ylab='Average Accuracy Scores (%)', frame=FALSE)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->

Quite a strange result. I supsect there is something *off* here, but
lets say that k = 7 nearest neighbours then is the best accuracy. Most
choices of k in fact have an accuracy of 76%, but simplicity wins out in
modelling.

## Support Vector Machine (SVM)

Possibly a good model for prediction, but need to fine tune some
parameters. Lets try it first.

``` r
## svm
svm.model <- svm(scaled.train[,301] ~ .,data = scaled.train)
svm.pred <- predict(svm.model, scaled.test)
# Have a look at both predictions and actual target data
svm.pred
```

    ##          1          2          3          4          5          6          7 
    ## 0.19439405 0.10901505 0.18334939 0.17880583 0.35759363 0.16911668 0.38392794 
    ##          8          9         10         11         12         13         14 
    ## 0.17967645 0.19009525 0.39817174 0.18152699 0.06661878 0.21559133 0.34879222 
    ##         15         16         17         18         19         20         21 
    ## 0.20005922 0.15129314 0.13794792 0.25726533 0.23039279 0.29813571 0.21412365 
    ##         22         23         24         25 
    ## 0.36942972 0.29323062 0.33545778 0.22597433

``` r
scaled.test[,301]
```

    ##  [1] 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0

``` r
# Seems like we could guess a cutoff (not very accurate).
```

So approximating the cutoff value for binary prediction of the target
variable is required. We can just iterate over a sequence and examine
the accuracy. A simple function is a good call.

``` r
# Find a function to define a cutoff value
cutoff_fn <- function(z){
  sp <- ifelse(svm.pred > z,1,0) # Apply the cutoff
  sp_tab <- table(sp,scaled.test[,301]) # Tabulate the values
  # Calculate the total it got correct
  correct <- sum(diag(sp_tab))/sum(sp_tab)
}
z <- seq(0.1, 1, by=0.05)
accuracy <- sapply(z, cutoff_fn)

plot(z, type='b', accuracy, xlab='Cutoff Value for Prediction', ylab='Accuracy Score (%)', frame=FALSE)
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-18-1.png)<!-- -->

It seems as though a cutoff of 0.45 might be the optimum cutoff value.
What about the *cost* function in the svm method?

``` r
# Create a dataframe and store the next function's results
df <- data.frame(matrix(ncol = 3, nrow = length(z)))
x <- c("0.1", "1", "10")
colnames(df) <- x


# Now try to change the cost value in the svm() function
costvector <- c(0.1,1,1000)
#accuracy <- sapply(z, cutoff_fn)

for (i in 1:length(costvector)) {
  svm.model <- svm(scaled.train[,301] ~ .,data = scaled.train,cost = i)
  svm.pred <- predict(svm.model, scaled.test)
  
  accuracy <- sapply(z, cutoff_fn)
  df[,i] <- accuracy
}


plot(z, type='b', df[,1], xlab='Cutoff Value for Prediction', ylab='Accuracy Score (%)', frame=FALSE)
lines(z, df[,2], col="red",type='b')
lines(z, df[,3], col="blue",type = 'b')
```

![](1.-Import-and-Explore_files/figure-gfm/unnamed-chunk-19-1.png)<!-- -->

So lets go with a cost of 1 and a prediction cut off of 0.45. To further
stress test the model, lets do k fold cross validation.

## K fold (10) Cross Validation

Explained
[here](http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/),
"The k-fold cross-validation method evaluates the model performance on
different subset of the training data and then calculate the average
prediction error rate. The algorithm is as follows:

1.  Randomly split the data set into k-subsets (or k-fold) (for example
    5 subsets)
2.  Reserve one subset and train the model on all other subsets
3.  Test the model on the reserved subset and record the prediction
    error
4.  Repeat this process until each of the k subsets has served as the
    test set.
5.  Compute the average of the k recorded errors. This is called the
    cross-validation error serving as the performance metric for the
    model.

K-fold cross-validation (CV) is a robust method for estimating the
accuracy of a model. I usually prefer to do it pseudo-manually so that I
can see each fold.

In the following chunk of code, we create the folds, divide up the
original (scaled) data into train and test again (for each of the 10
folds) and run the SVM algorithm using a cost value of 5 and a
prediction cutoff of 0.45, as determined above.

``` r
set.seed(121)
flds <- createFolds(1:nrow(scaled.dat), k = 10, list = TRUE, returnTrain = FALSE)

# Create an empty vector to store the accuracy in
result <- c()

# Run the SVM model for each fold, where each fold indexing is the test set
for (i in 1:10) {
  # Get the new training and test sets for current fold
  new.test.set <- scaled.dat[unlist(flds[i],use.names = FALSE),]
  new.train.set <- scaled.dat[-unlist(flds[i],use.names = FALSE),]
  
  # Run the model for the values found above
  svm.mod <- svm(new.train.set[,301] ~ .,data = new.train.set,cost = 5)
  svm.pred <- predict(svm.mod, new.test.set)
  
  # Now apply the cutoff value of 0.45
  sp <- ifelse(svm.pred > 0.45,1,0) # Apply the cutoff
  sp_tab <- table(sp,new.test.set[,301]) # Tabulate the values
  # Calculate the total it got correct
  #correct <- sum(diag(sp_tab))/sum(sp_tab)
  result[i] <- sum(diag(sp_tab))/sum(sp_tab)
}

result
```

    ##  [1] 0.9200000 0.8400000 0.7600000 0.7777778 0.7916667 0.8750000 0.8076923
    ##  [8] 0.9230769 0.9166667 0.9583333

``` r
mean(result)
```

    ## [1] 0.8570214

This is not a bad level of accuracy\! Probably enough for now to try and
submit to Kaggle to see how we do on the (much) larger & unseen
dataset\!
