---
title: "Import and Explore"
author: "Eoin Scanlon"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(grid)
library(gridExtra)
#install.packages("MVN")
library(MVN)
library(reshape2)
library(cluster) 
library(factoextra)
library(class)
library(e1071)
library(rpart)
library(caret)
```

# Import and sneak peak

Data can be downloaded from <https://www.kaggle.com/c/dont-overfit-ii/data>. Always good to have a look at the structure of the data first.

```{r}
train <- read.csv(file = 'train.csv')
#head(train)
ncol(train)
nrow(train)
```

The column 'target' is the outcome that we will wish to predict, and the rest are just arbitrary, meaningless data. The training dataset is much, much smaller than the test. Interesting. Lets see some summary statistics of the data.

```{r}
#summary(train)
```

This is very hard to read, so lets see how to interpret this a bit better.

```{r}
train.data <- train[,-c(1,2)] # Remove unnecessary columns
train.means <- colMeans(train.data) # Means of each column
train.sd <- apply(train.data, 2, sd) # SD of each column

p1<- ggplot(as.data.frame(train.means), aes(x = train.means)) + geom_histogram(color="black", fill="blue",binwidth = 0.01)
p2<- ggplot(as.data.frame(train.sd), aes(x = train.sd)) + geom_histogram(color="black", fill="green",binwidth = 0.01)

grid.arrange(p1, p2, ncol = 2)

```

Ok so the means seem to be spread from about -0.2 to +0.2, with a normal distribution type vibe. So lets draw a normal distribution over the plots and see.

```{r}
# Means
p1 <- ggplot(as.data.frame(train.means), aes(x = train.means)) + geom_histogram(aes(y =..density..),
                                                                          color="black", 
                                                                          fill="blue",
                                                                          binwidth = 0.01) + 
  stat_function(fun = dnorm, args = list(mean = mean(train.means), sd = sd(train.means)))

# Standard Deviations
p2 <- ggplot(as.data.frame(train.sd), aes(x = train.sd)) + geom_histogram(aes(y =..density..),
                                                                    color="black", 
                                                                    fill="green",
                                                                    binwidth = 0.01) +
  stat_function(fun = dnorm, args = list(mean = mean(train.sd), sd = sd(train.sd)))

grid.arrange(p1, p2, ncol = 2)

```

## Testing for Normality

We can test whether these data are normally distributed using QQplots.
```{r}
# Test for normality
qqnorm(train.means,main="QQ plot of means",pch=19)
qqline(train.means)

qqnorm(train.sd,main="QQ plot of SDs",pch=19)
qqline(train.sd)

```

They seem to be pretty consistent with normal distributions. But these are just the *means* and the *standard deviations*. If we want to look at the actual data, we should use something non-visual, since there are so may columns. The Shapiro-Wilk test might be a good idea for single column inputs, but here we can use a multivariate normality test.

```{r, results="hide"}
mvn(train.data,multivariatePlot = "qq")
```

So clearly, the data is not multivariate normal, or the points above would lie along the straight line. 

## Correlation Analysis

```{r}
res <- cor(train.data)
#round(res, 2)
melted_data <- melt(res)
ggplot(data = melted_data, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile()

```

At this scale, this looks like pure white noise. There may be correlations within this data, but we just can't see them here because of the size issue. So lets look at a histogram of all the correlations.

```{r}
head(melted_data)
ggplot(melted_data, aes(x = melted_data$value)) + geom_histogram(aes(y =..density..),
                                                                    color="black", 
                                                                    fill="yellow",
                                                                    binwidth = 0.01) + 
  stat_function(fun = dnorm, args = list(mean = mean(melted_data$value), sd = sd(melted_data$value)))

```

Pretty safe to say so that all of the data is uncorrelated. The correlation values are almost a textbook normal distrubution with a mean of 0 and small standard deviation. The small spike at 1 is the values' self-correlations.

So lets draw a line under this now and assume (safely) that there is very little correlation between columns, the data is not multivariate normal and that the means/standard deviations *between* columns are normally distributed.

# Further Data Exploration

First up, standardise the data. Its not really necessary, as the data is so similar in terms of range. But still, can't hurt.

```{r}
scaled.dat <- scale(train.data)
```

## PCA

Lets look at a principle component analysis. Since the dataset is so large (300 columns/variables), we would hope that we'll discover that a small number of components (eigenvectors) account for a significant proportion of the data.

```{r}
dat.pca <- prcomp(scaled.dat)
x <- summary(dat.pca)
x$importance[,1:10]
```

No such luck. It seems as though all 300 components are equally 'important'. There doesn't seem to be much here in terms of eigenvector decomposition. From the output above, we can see that the cumulative population of even the top 10 components accounts for only around 13% of all variance in the data. Hardly useful.

## K-means clustering

We will have to first figure out the 'correct' (optimum) number of clusters (k). To do so, we can implement a simple function that iterates over a range of k and calculates their sihouette scores.

```{r}
silhouette_score <- function(k){
  km <- kmeans(scaled.dat, centers = k, nstart=25)
  ss <- silhouette(km$cluster, dist(scaled.dat))
  mean(ss[,3])
}
```

Run and plot the average silhouette scores.

```{r}
k <- 2:20
avg_sil <- sapply(k, silhouette_score)
plot(k, type='b', avg_sil, xlab='Number of clusters', ylab='Average Silhouette Scores', frame=FALSE)
```

It seems as though 16 may be the optimum, although that is only relative. The actual silhouette score is low for k = 16, at around 0.012. This data set is really weird. Another method gives us the same uncertainty.

```{r}
fviz_nbclust(scaled.dat, kmeans, method='silhouette',k.max = 20)
```

However, model simplicity should always be favoured over complexity. I won't pursue the clustering method, as > 16 or so clusters, with marginally increasing signs of improvement leaves me with a bad feeling.

## K Nearest Neighbours (KNN)

As this is a supervised algorithm, we'll need to break this training dataset of 250 rows into its own train and test first.

```{r}
set.seed(123)
# Add back in the target variable first
scaled.dat <- cbind(scaled.dat,train$target)

# Break up the data into train and test 
## Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(scaled.dat), 0.9 * nrow(scaled.dat)) 

## Extract training set
scaled.train <- scaled.dat[ran,] 
## Extract testing set
scaled.test <- scaled.dat[-ran,] 

```

Now run the k nearest neighbours for, say, 13 nearest neighbours. We'll try to optimise this next.

```{r}
pr <- knn(scaled.train[,0:300],scaled.test[,0:300],cl=scaled.train[,301],k=13)

##create confusion matrix
tab <- table(pr,scaled.test[,301])

##this function divides the correct predictions by total number of predictions that tell us how accurate the model is.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)

```

This gives about 75% accuracy. But maybe there is better with a different choice of k. Lets create a function similar to that in the clustering above.

```{r}
vector = c() # This is the empty vector where we will store our scores

neighbour_score <- function(k){
  pr <- knn(scaled.train[,0:300],scaled.test[,0:300],cl=scaled.train[,301],k)
  tab <- table(pr,scaled.test[,301])
  vector <- c(accuracy(tab))
}
k <- 2:20
avg_neighbour <- sapply(k, neighbour_score)
plot(k, type='b', avg_neighbour, xlab='Number of neighbours', ylab='Average Accuracy Scores (%)', frame=FALSE)

```

Quite a strange result. I supsect there is something *off* here, but lets say that k = 7 nearest neighbours then is the best accuracy. Most choices of k in fact have an accuracy of 76%, but simplicity wins out in modelling.


## Support Vector Machine (SVM)

Possibly a good model for prediction, but need to fine tune some parameters. Lets try it first.

```{r}
## svm
svm.model <- svm(scaled.train[,301] ~ .,data = scaled.train)
svm.pred <- predict(svm.model, scaled.test)
# Have a look at both predictions and actual target data
svm.pred
scaled.test[,301]
# Seems like we could guess a cutoff (not very accurate).

```

So approximating the cutoff value for binary prediction of the target variable is required. We can just iterate over a sequence and examine the accuracy. A simple function is a good call.

```{r}
# Find a function to define a cutoff value
cutoff_fn <- function(z){
  sp <- ifelse(svm.pred > z,1,0) # Apply the cutoff
  sp_tab <- table(sp,scaled.test[,301]) # Tabulate the values
  # Calculate the total it got correct
  correct <- sum(diag(sp_tab))/sum(sp_tab)
}
z <- seq(0.1, 1, by=0.05)
accuracy <- sapply(z, cutoff_fn)

plot(z, type='b', accuracy, xlab='Cutoff Value for Prediction', ylab='Accuracy Score (%)', frame=FALSE)
```

It seems as though a cutoff of 0.45 might be the optimum cutoff value. What about the *cost* function in the svm method?

```{r}
# Create a dataframe and store the next function's results
df <- data.frame(matrix(ncol = 3, nrow = length(z)))
x <- c("0.1", "1", "10")
colnames(df) <- x


# Now try to change the cost value in the svm() function
costvector <- c(0.1,1,1000)
#accuracy <- sapply(z, cutoff_fn)

for (i in 1:length(costvector)) {
  svm.model <- svm(scaled.train[,301] ~ .,data = scaled.train,cost = i)
  svm.pred <- predict(svm.model, scaled.test)
  
  accuracy <- sapply(z, cutoff_fn)
  df[,i] <- accuracy
}


plot(z, type='b', df[,1], xlab='Cutoff Value for Prediction', ylab='Accuracy Score (%)', frame=FALSE)
lines(z, df[,2], col="red",type='b')
lines(z, df[,3], col="blue",type = 'b')

```

So lets go with a cost of 1 and a prediction cut off of 0.45. To further stress test the model, lets do k fold cross validation.

## K fold (10) Cross Validation

Explained [here](http://www.sthda.com/english/articles/38-regression-model-validation/157-cross-validation-essentials-in-r/), "The k-fold cross-validation method evaluates the model performance on different subset of the training data and then calculate the average prediction error rate. The algorithm is as follows:

1. Randomly split the data set into k-subsets (or k-fold) (for example 5 subsets)
2. Reserve one subset and train the model on all other subsets
3. Test the model on the reserved subset and record the prediction error
4. Repeat this process until each of the k subsets has served as the test set.
5. Compute the average of the k recorded errors. This is called the cross-validation error serving as the performance metric for the model.

K-fold cross-validation (CV) is a robust method for estimating the accuracy of a model.
